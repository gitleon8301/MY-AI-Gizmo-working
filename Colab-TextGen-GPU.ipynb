{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MY-AI-GIZMO WORKING COLAB LAUNCHER - FIXED FOR REAL\n",
        "# No llama_cpp_binaries dependency + GPU/CPU auto-detection\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üöÄ MY-AI-GIZMO WORKING INSTALLER & LAUNCHER\")\n",
        "print(\"=\"*70)\n",
        "print(\"Fixed: No llama_cpp_binaries dependency + GPU/CPU auto-detection\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "def run_cmd(cmd, check=False, quiet=True, timeout=None):\n",
        "    \"\"\"Run command safely\"\"\"\n",
        "    try:\n",
        "        return subprocess.run(cmd, check=check, capture_output=quiet,\n",
        "                            text=True, timeout=timeout)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def safe_pip_install(packages):\n",
        "    \"\"\"Install pip packages\"\"\"\n",
        "    if isinstance(packages, str):\n",
        "        packages = [packages]\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                         check=False, timeout=300,\n",
        "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Step 1: Mount Drive\n",
        "print(\"\\nüìÇ Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"   ‚úÖ Mounted\")\n",
        "except:\n",
        "    print(\"   ‚ö†Ô∏è  Mount failed, continuing...\")\n",
        "\n",
        "# Create directories\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Step 2: System packages\n",
        "print(\"\\nüì¶ Installing system packages...\")\n",
        "run_cmd([\"apt-get\", \"update\", \"-qq\"])\n",
        "run_cmd([\"apt-get\", \"install\", \"-y\", \"-qq\", \"build-essential\", \"cmake\", \"git\", \"wget\"])\n",
        "print(\"   ‚úÖ Done\")\n",
        "\n",
        "# Step 3: Clone repo\n",
        "print(\"\\nüì• Setting up repository...\")\n",
        "if REPO_DIR.exists():\n",
        "    os.chdir(REPO_DIR)\n",
        "    run_cmd([\"git\", \"pull\"])\n",
        "else:\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    os.chdir(REPO_DIR.parent)\n",
        "    run_cmd([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"])\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"   ‚úÖ Ready at {REPO_DIR}\")\n",
        "\n",
        "# Step 4: Python packages\n",
        "print(\"\\nüêç Installing Python packages...\")\n",
        "safe_pip_install(\"pip --upgrade\")\n",
        "safe_pip_install([\"setuptools\", \"wheel\", \"numpy\", \"requests\", \"tqdm\", \"pyyaml\"])\n",
        "\n",
        "# Install llama-cpp-python with server support\n",
        "print(\"   Installing llama-cpp-python with server...\")\n",
        "env = dict(os.environ)\n",
        "env[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=off -DLLAMA_BUILD_SERVER=ON\"\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "               \"llama-cpp-python[server]\", \"--no-cache-dir\", \"-q\"],\n",
        "              check=False, env=env, timeout=600)\n",
        "\n",
        "safe_pip_install([\"torch\", \"transformers\", \"gradio>=3.50.0\", \"accelerate\",\n",
        "                 \"markdown\", \"Pillow\", \"safetensors\", \"sentencepiece\",\n",
        "                 \"protobuf\", \"flask\", \"flask-cloudflared\"])\n",
        "print(\"   ‚úÖ Packages installed\")\n",
        "\n",
        "# Step 5: THE CRITICAL PATCH - Replace llama_cpp_server.py completely\n",
        "print(\"\\nüîß Patching llama_cpp_server.py...\")\n",
        "\n",
        "FIXED_LLAMA_SERVER = '''import json\n",
        "import torch\n",
        "import os\n",
        "import pprint\n",
        "import re\n",
        "import socket\n",
        "import subprocess\n",
        "import sys\n",
        "import threading\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Any, List\n",
        "import requests\n",
        "\n",
        "from modules import shared\n",
        "from modules.image_utils import (\n",
        "    convert_image_attachments_to_pil,\n",
        "    convert_openai_messages_to_images,\n",
        "    convert_pil_to_base64\n",
        ")\n",
        "from modules.logging_colors import logger\n",
        "from modules.utils import resolve_model_path\n",
        "\n",
        "llamacpp_valid_cache_types = {\"fp16\", \"q8_0\", \"q4_0\"}\n",
        "\n",
        "def get_llama_cpp_server_path():\n",
        "    \"\"\"Find llama-server executable - NO llama_cpp_binaries dependency\"\"\"\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        possible_paths = [\n",
        "            Path(llama_cpp.__file__).parent / \"server\" / \"llama-server\",\n",
        "            Path(sys.prefix) / \"bin\" / \"llama-server\",\n",
        "            Path(\"/usr/local/bin/llama-server\"),\n",
        "        ]\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                logger.info(f\"Found llama-server at: {path}\")\n",
        "                return str(path)\n",
        "\n",
        "        # Try system PATH\n",
        "        result = subprocess.run(['which', 'llama-server'],\n",
        "                              capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            return result.stdout.strip()\n",
        "\n",
        "        logger.warning(\"Using llama-server from PATH\")\n",
        "        return \"llama-server\"\n",
        "    except:\n",
        "        return \"llama-server\"\n",
        "\n",
        "class LlamaServer:\n",
        "    def __init__(self, model_path, server_path=None):\n",
        "        self.model_path = model_path\n",
        "        self.server_path = server_path\n",
        "        self.port = self._find_available_port()\n",
        "        self.process = None\n",
        "        self.session = requests.Session()\n",
        "        self.vocabulary_size = None\n",
        "        self.bos_token = \"~~\"\n",
        "        self.last_prompt_token_count = 0\n",
        "        self._start_server()\n",
        "\n",
        "    def encode(self, text, add_bos_token=False, **kwargs):\n",
        "        if self.bos_token and text.startswith(self.bos_token):\n",
        "            add_bos_token = False\n",
        "        url = f\"http://127.0.0.1:{self.port}/tokenize\"\n",
        "        response = self.session.post(url, json={\"content\": text, \"add_special\": add_bos_token})\n",
        "        return response.json().get(\"tokens\", [])\n",
        "\n",
        "    def decode(self, token_ids, **kwargs):\n",
        "        url = f\"http://127.0.0.1:{self.port}/detokenize\"\n",
        "        response = self.session.post(url, json={\"tokens\": token_ids})\n",
        "        return response.json().get(\"content\", \"\")\n",
        "\n",
        "    def prepare_payload(self, state):\n",
        "        payload = {\n",
        "            \"temperature\": state[\"temperature\"] if not state[\"dynamic_temperature\"] else (state[\"dynatemp_low\"] + state[\"dynatemp_high\"]) / 2,\n",
        "            \"top_k\": state[\"top_k\"],\n",
        "            \"top_p\": state[\"top_p\"],\n",
        "            \"min_p\": state[\"min_p\"],\n",
        "            \"repeat_penalty\": state[\"repetition_penalty\"],\n",
        "            \"seed\": state[\"seed\"],\n",
        "        }\n",
        "        return payload\n",
        "\n",
        "    def _process_images_for_generation(self, state: dict) -> List[Any]:\n",
        "        pil_images = []\n",
        "        if 'image_attachments' in state and state['image_attachments']:\n",
        "            pil_images.extend(convert_image_attachments_to_pil(state['image_attachments']))\n",
        "        return pil_images\n",
        "\n",
        "    def is_multimodal(self) -> bool:\n",
        "        return shared.args.mmproj not in [None, 'None']\n",
        "\n",
        "    def generate_with_streaming(self, prompt, state):\n",
        "        url = f\"http://127.0.0.1:{self.port}/completion\"\n",
        "        payload = self.prepare_payload(state)\n",
        "\n",
        "        token_ids = self.encode(prompt, add_bos_token=state.get(\"add_bos_token\", False))\n",
        "        self.last_prompt_token_count = len(token_ids)\n",
        "        payload[\"prompt\"] = token_ids\n",
        "\n",
        "        max_new_tokens = state.get('max_new_tokens', 200)\n",
        "        payload.update({\"n_predict\": max_new_tokens, \"stream\": True, \"cache_prompt\": True})\n",
        "\n",
        "        response = self.session.post(url, json=payload, stream=True)\n",
        "        full_text = \"\"\n",
        "\n",
        "        try:\n",
        "            for line in response.iter_lines():\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    line = line.decode('utf-8')\n",
        "                    if line.startswith('data: '):\n",
        "                        line = line[6:]\n",
        "                    data = json.loads(line)\n",
        "                    if data.get('content'):\n",
        "                        full_text += data['content']\n",
        "                        yield full_text\n",
        "                    if data.get('stop'):\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "        finally:\n",
        "            response.close()\n",
        "\n",
        "    def generate(self, prompt, state):\n",
        "        output = \"\"\n",
        "        for output in self.generate_with_streaming(prompt, state):\n",
        "            pass\n",
        "        return output\n",
        "\n",
        "    def get_logits(self, prompt, state, n_probs=128, use_samplers=False):\n",
        "        url = f\"http://127.0.0.1:{self.port}/completion\"\n",
        "        payload = self.prepare_payload(state)\n",
        "        payload.update({\n",
        "            \"prompt\": self.encode(prompt),\n",
        "            \"n_predict\": 0,\n",
        "            \"logprobs\": True,\n",
        "            \"n_probs\": n_probs,\n",
        "            \"stream\": False,\n",
        "        })\n",
        "        response = self.session.post(url, json=payload)\n",
        "        result = response.json()\n",
        "        if \"completion_probabilities\" in result:\n",
        "            return result[\"completion_probabilities\"][0][\"top_probs\" if use_samplers else \"top_logprobs\"]\n",
        "        raise Exception(\"Unexpected response format\")\n",
        "\n",
        "    def _get_vocabulary_size(self):\n",
        "        url = f\"http://127.0.0.1:{self.port}/v1/models\"\n",
        "        response = self.session.get(url).json()\n",
        "        if \"data\" in response and len(response[\"data\"]) > 0:\n",
        "            model_info = response[\"data\"][0]\n",
        "            if \"meta\" in model_info and \"n_vocab\" in model_info[\"meta\"]:\n",
        "                self.vocabulary_size = model_info[\"meta\"][\"n_vocab\"]\n",
        "\n",
        "    def _get_bos_token(self):\n",
        "        url = f\"http://127.0.0.1:{self.port}/props\"\n",
        "        response = self.session.get(url).json()\n",
        "        if \"bos_token\" in response:\n",
        "            self.bos_token = response[\"bos_token\"]\n",
        "\n",
        "    def _find_available_port(self):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.bind(('', 0))\n",
        "            return s.getsockname()[1]\n",
        "\n",
        "    def _start_server(self):\n",
        "        if self.server_path is None:\n",
        "            self.server_path = get_llama_cpp_server_path()\n",
        "\n",
        "        # GPU AUTO-DETECTION\n",
        "        has_gpu = torch.cuda.is_available()\n",
        "        gpu_layers = shared.args.gpu_layers if has_gpu else 0\n",
        "\n",
        "        if has_gpu:\n",
        "            logger.info(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            logger.warning(\"No GPU ‚Üí CPU mode\")\n",
        "\n",
        "        cmd = [\n",
        "            self.server_path,\n",
        "            \"--model\", self.model_path,\n",
        "            \"--ctx-size\", str(shared.args.ctx_size),\n",
        "            \"--batch-size\", str(shared.args.batch_size),\n",
        "            \"--port\", str(self.port),\n",
        "            \"--no-webui\",\n",
        "        ]\n",
        "\n",
        "        # GPU flags ONLY if GPU exists\n",
        "        if has_gpu and gpu_layers > 0:\n",
        "            cmd += [\"--gpu-layers\", str(gpu_layers)]\n",
        "\n",
        "        if shared.args.threads > 0:\n",
        "            cmd += [\"--threads\", str(shared.args.threads)]\n",
        "\n",
        "        env = os.environ.copy()\n",
        "\n",
        "        logger.info(f\"llama.cpp | GPU={has_gpu} | layers={gpu_layers} | ctx={shared.args.ctx_size}\")\n",
        "\n",
        "        self.process = subprocess.Popen(cmd, stderr=subprocess.PIPE, bufsize=0, env=env)\n",
        "\n",
        "        threading.Thread(target=self._log_stderr, daemon=True).start()\n",
        "\n",
        "        # Wait for health\n",
        "        health_url = f\"http://127.0.0.1:{self.port}/health\"\n",
        "        for _ in range(60):\n",
        "            if self.process.poll() is not None:\n",
        "                raise RuntimeError(f\"Server exited with code {self.process.returncode}\")\n",
        "            try:\n",
        "                if self.session.get(health_url, timeout=1).status_code == 200:\n",
        "                    break\n",
        "            except:\n",
        "                pass\n",
        "            time.sleep(1)\n",
        "\n",
        "        self._get_vocabulary_size()\n",
        "        self._get_bos_token()\n",
        "        return self.port\n",
        "\n",
        "    def _log_stderr(self):\n",
        "        try:\n",
        "            for line in iter(self.process.stderr.readline, b''):\n",
        "                print(line.decode('utf-8', errors='replace').strip(), file=sys.stderr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def stop(self):\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            try:\n",
        "                self.process.wait(timeout=5)\n",
        "            except:\n",
        "                self.process.kill()\n",
        "            self.process = None\n",
        "\n",
        "    def __del__(self):\n",
        "        self.stop()\n",
        "'''\n",
        "\n",
        "llama_server_file = REPO_DIR / \"modules\" / \"llama_cpp_server.py\"\n",
        "llama_server_file.write_text(FIXED_LLAMA_SERVER, encoding='utf-8')\n",
        "print(\"   ‚úÖ llama_cpp_server.py completely replaced (NO llama_cpp_binaries)\")\n",
        "\n",
        "# Step 6: Link user_data\n",
        "print(\"\\nüîó Linking user_data...\")\n",
        "local_user_data = REPO_DIR / \"user_data\"\n",
        "if local_user_data.exists() and not local_user_data.is_symlink():\n",
        "    shutil.rmtree(local_user_data)\n",
        "if not local_user_data.exists():\n",
        "    local_user_data.symlink_to(USER_DATA_DIR)\n",
        "(USER_DATA_DIR / \"logs\").mkdir(parents=True, exist_ok=True)\n",
        "print(\"   ‚úÖ Linked\")\n",
        "\n",
        "# Step 7: Download model\n",
        "print(f\"\\n‚¨áÔ∏è  Checking model...\")\n",
        "if MODEL_FILE.exists():\n",
        "    print(f\"   ‚úÖ Model exists ({MODEL_FILE.stat().st_size/(1024**3):.2f} GB)\")\n",
        "else:\n",
        "    print(\"   üì• Downloading model (3.8 GB, one-time)...\")\n",
        "    MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "    run_cmd([\"wget\", \"-q\", \"--show-progress\", \"-O\", str(MODEL_FILE),\n",
        "            \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"],\n",
        "           timeout=900)\n",
        "    if MODEL_FILE.exists():\n",
        "        print(f\"   ‚úÖ Downloaded ({MODEL_FILE.stat().st_size/(1024**3):.2f} GB)\")\n",
        "\n",
        "# Step 8: Link models\n",
        "print(\"\\nüîó Linking models...\")\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "print(\"   ‚úÖ Linked\")\n",
        "\n",
        "# Step 9: Detect GPU and build command\n",
        "print(\"\\nüñ•Ô∏è  Detecting hardware...\")\n",
        "has_gpu = False\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    if has_gpu:\n",
        "        print(f\"   ‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"   ‚ÑπÔ∏è  No GPU, using CPU\")\n",
        "except:\n",
        "    print(\"   ‚ÑπÔ∏è  Using CPU\")\n",
        "\n",
        "cmd = [sys.executable, \"server.py\"]\n",
        "\n",
        "if has_gpu:\n",
        "    cmd.extend([\"--gpu-layers\", \"35\"])\n",
        "else:\n",
        "    cmd.extend([\"--cpu\"])\n",
        "\n",
        "cmd.extend([\n",
        "    \"--threads\", \"4\",\n",
        "    \"--listen\",\n",
        "    \"--listen-host\", \"0.0.0.0\",\n",
        "    \"--share\",\n",
        "    \"--model\", str(MODEL_FILE),\n",
        "    \"--loader\", \"llama.cpp\"\n",
        "])\n",
        "\n",
        "print(f\"\\nüìù Configuration:\")\n",
        "print(f\"   Hardware: {'GPU' if has_gpu else 'CPU'}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "print(f\"   Command: {' '.join(str(x) for x in cmd[:8])}...\")\n",
        "\n",
        "# Step 10: LAUNCH!\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ LAUNCHING SERVER - WATCH FOR GRADIO LINK BELOW!\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "try:\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                              universal_newlines=True, bufsize=1)\n",
        "\n",
        "    found_link = False\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "        if ('gradio.live' in line.lower() or\n",
        "            ('running on' in line.lower() and 'http' in line)) and not found_link:\n",
        "            print(\"\\n\" + \"üéâ\"*35)\n",
        "            print(\"‚úÖ SERVER IS RUNNING! CLICK THE LINK ABOVE ‚òùÔ∏è\")\n",
        "            print(\"üéâ\"*35)\n",
        "            print(f\"\\nüí° Tips:\")\n",
        "            print(f\"   ‚Ä¢ Click the Gradio link\")\n",
        "            print(f\"   ‚Ä¢ Hardware: {'GPU' if has_gpu else 'CPU'}\")\n",
        "            print(f\"   ‚Ä¢ Chats save to: {USER_DATA_DIR / 'logs'}\")\n",
        "            print(f\"   ‚Ä¢ Press Ctrl+C to stop\\n\")\n",
        "            found_link = True\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚èπÔ∏è  Stopped by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Done!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}