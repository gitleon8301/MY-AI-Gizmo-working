{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KListra in HELA den här cellen i Colab och kör\n",
        "import os, shutil, subprocess, sys, time\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- USER-EDITABLE ----------\n",
        "GIT_URL = \"https://github.com/gitleon8301/MY-AI-Gizmo-working\"\n",
        "REPO_DIR_NAME = \"text-generation-webui\"   # where the start script lives after cloning\n",
        "MODEL_REPO = \"google/gemma-2-9b-it-GGUF\"  # CPU-compatible repo (GGUF)\n",
        "SPECIFIC_FILE = \"gemma-2-9b-it-Q4_K_M.gguf\"  # recommended CPU quant\n",
        "# If the repo requires authentication for large files, set HF_TOKEN in Colab runtime env:\n",
        "# os.environ[\"HF_TOKEN\"] = \"hf_...\"  (or set via Colab UI: Runtime -> Change runtime variables)\n",
        "# ------------------------------------\n",
        "\n",
        "# logs\n",
        "DOWNLOAD_LOG = Path(\"download.log\")\n",
        "START_LOG = Path(\"webui.log\")\n",
        "\n",
        "# ensure deterministic paths\n",
        "cwd = Path.cwd()\n",
        "repo_dir = cwd / REPO_DIR_NAME\n",
        "models_dir = cwd / \"models\" / MODEL_REPO.replace(\"/\", \"_\")\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# force CPU only\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "os.environ.pop('MPLBACKEND', None)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # hide GPUs\n",
        "\n",
        "def run(cmd, cwd=None, env=None, logfile=None, check=False):\n",
        "    print(f\"\\n>>> RUN: {cmd}\")\n",
        "    with subprocess.Popen(cmd, shell=True, cwd=cwd, env=env or os.environ,\n",
        "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True) as p:\n",
        "        out_lines = []\n",
        "        for line in p.stdout:\n",
        "            out_lines.append(line)\n",
        "            if logfile:\n",
        "                with open(logfile, \"a\") as f:\n",
        "                    f.write(line)\n",
        "            # also print a subset in realtime so user sees progress\n",
        "            print(line, end=\"\")\n",
        "        p.wait()\n",
        "        if check and p.returncode != 0:\n",
        "            raise subprocess.CalledProcessError(p.returncode, cmd)\n",
        "        return p.returncode, \"\".join(out_lines)\n",
        "\n",
        "# 1) Clone the repo if missing\n",
        "if not repo_dir.exists():\n",
        "    print(\"Cloning web-UI repo...\")\n",
        "    run(f\"git clone {GIT_URL} {REPO_DIR_NAME}\", check=True)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# 2) Run install script but do not auto-launch\n",
        "print(\"\\nRunning install (LAUNCH_AFTER_INSTALL=FALSE)...\")\n",
        "try:\n",
        "    run('GPU_CHOICE=A LAUNCH_AFTER_INSTALL=FALSE INSTALL_EXTENSIONS=FALSE ./start_linux.sh', check=False, logfile=START_LOG)\n",
        "except Exception as e:\n",
        "    print(\"Install script returned non-zero, continuing (some installs print non-zero).\")\n",
        "\n",
        "# 3) Clear HF cache to avoid 416 partial-range issues\n",
        "hf_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
        "if hf_cache.exists():\n",
        "    print(\"Removing Hugging Face cache to avoid Range/partial-file issues...\")\n",
        "    try:\n",
        "        shutil.rmtree(hf_cache)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to remove HF cache:\", e)\n",
        "\n",
        "# 4) Attempt to download the specific GGUF file using download-model.py with retries\n",
        "download_success = False\n",
        "attempts = 3\n",
        "for attempt in range(1, attempts+1):\n",
        "    print(f\"\\nDownload attempt {attempt}/{attempts} using download-model.py (specific file)...\")\n",
        "    # ensure previous log truncated on first attempt\n",
        "    if attempt == 1 and DOWNLOAD_LOG.exists():\n",
        "        DOWNLOAD_LOG.unlink()\n",
        "    cmd = f\"python download-model.py {MODEL_REPO} --specific-file {SPECIFIC_FILE}\"\n",
        "    ret, out = run(cmd, logfile=DOWNLOAD_LOG)\n",
        "    if ret == 0:\n",
        "        print(\"download-model.py succeeded.\")\n",
        "        download_success = True\n",
        "        break\n",
        "    else:\n",
        "        print(f\"download-model.py failed (exit {ret}). Sleeping before retry...\")\n",
        "        time.sleep(5 * attempt)\n",
        "\n",
        "# 5) If download-model.py failed, try huggingface_hub.fallback (more robust)\n",
        "if not download_success:\n",
        "    print(\"\\nPrimary downloader failed. Trying direct download via huggingface_hub.hf_hub_download fallback...\")\n",
        "    # ensure huggingface_hub is installed\n",
        "    run(\"pip install -q huggingface_hub\", logfile=DOWNLOAD_LOG)\n",
        "    try:\n",
        "        from huggingface_hub import hf_hub_download\n",
        "        repo_id = MODEL_REPO\n",
        "        filename = SPECIFIC_FILE\n",
        "        target_path = models_dir / filename\n",
        "        if target_path.exists():\n",
        "            print(\"File already exists:\", target_path)\n",
        "            download_success = True\n",
        "        else:\n",
        "            # If the model repo is public, hf_hub_download will work without token. If private, set HF_TOKEN env variable.\n",
        "            hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "            if hf_token:\n",
        "                print(\"Using HF_TOKEN from environment.\")\n",
        "            print(f\"Downloading {filename} from repo {repo_id} to {target_path} ...\")\n",
        "            # hf_hub_download may stream; wrap in try/except\n",
        "            try:\n",
        "                local_file = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=str(models_dir), token=hf_token)\n",
        "                # move or ensure it is placed where we want\n",
        "                # hf_hub_download with cache_dir will put files under that cache; ensure a copy in desired dir\n",
        "                local_file_path = Path(local_file)\n",
        "                if local_file_path.exists():\n",
        "                    # move or copy to models_dir root (some hf_hub places nested folders)\n",
        "                    dest = models_dir / local_file_path.name\n",
        "                    if local_file_path.resolve() != dest.resolve():\n",
        "                        shutil.copy(local_file_path, dest)\n",
        "                    print(\"Downloaded to:\", dest)\n",
        "                    download_success = True\n",
        "                else:\n",
        "                    print(\"hf_hub_download did not create expected file.\")\n",
        "            except Exception as e:\n",
        "                print(\"hf_hub_download failed:\", repr(e))\n",
        "    except Exception as e:\n",
        "        print(\"Failed to import huggingface_hub or run fallback:\", repr(e))\n",
        "\n",
        "# 6) Final check: look for any .gguf file in models dir\n",
        "gguf_files = list(models_dir.glob(\"*.gguf\"))\n",
        "if not download_success and not gguf_files:\n",
        "    print(\"\\nERROR: model download still failed. Check download.log for details:\")\n",
        "    if DOWNLOAD_LOG.exists():\n",
        "        print(\"\\n--- Last 200 lines of download.log ---\")\n",
        "        run(f\"tail -n 200 {DOWNLOAD_LOG}\")\n",
        "    else:\n",
        "        print(\"download.log not found.\")\n",
        "    print(\"\\nPossible actions:\\n - Ensure MODEL_REPO and SPECIFIC_FILE are correct.\\n - If the repo is private, set HF_TOKEN environment variable (hf_... token) before running.\\n - Manually download the desired .gguf file from Hugging Face and upload it to the Colab session into `models/{MODEL_REPO.replace('/', '_')}`.\\n\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "print(\"\\nModel file present. Files in\", models_dir)\n",
        "run(f\"ls -lh {models_dir}\")\n",
        "\n",
        "# 7) Start the web UI in CPU mode and stream logs to webui.log\n",
        "print(\"\\nStarting the web UI in CPU mode. Logs appended to\", START_LOG)\n",
        "start_cmd = f\"./start_linux.sh --cpu --share --model {MODEL_REPO.replace('/', '_')}\"\n",
        "# Launch and stream\n",
        "try:\n",
        "    run(start_cmd, logfile=START_LOG, check=True)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Web UI failed with exit code {e.returncode}. See {START_LOG} for details.\")\n",
        "    run(f\"tail -n 200 {START_LOG}\")\n",
        "    raise\n",
        "\n",
        "# note: normally the web UI stays running (this cell will block while UI runs).\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}