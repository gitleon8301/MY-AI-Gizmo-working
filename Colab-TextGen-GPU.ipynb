{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MY-AI-GIZMO INTERACTIVE LAUNCHER WITH PERSISTENT STORAGE\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "print(\"üöÄ MY-AI-GIZMO INTERACTIVE INSTALLER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- STEP 1: Mount Google Drive ---\n",
        "print(\"\\nüìÇ Step 1: Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"   ‚úÖ Google Drive mounted\")\n",
        "\n",
        "# --- STEP 2: Setup Persistent Directories ---\n",
        "print(\"\\nüìÅ Step 2: Setting up persistent storage...\")\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "DRIVE_BASE.mkdir(exist_ok=True)\n",
        "\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "USER_DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"   üìÇ Installation: {DRIVE_BASE}\")\n",
        "\n",
        "already_installed = REPO_DIR.exists() and (REPO_DIR / \"server.py\").exists()\n",
        "model_exists = (MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\").exists()\n",
        "\n",
        "if already_installed:\n",
        "    print(\"   ‚úÖ Installation found\")\n",
        "if model_exists:\n",
        "    print(\"   ‚úÖ Model found\")\n",
        "\n",
        "# --- STEP 3: Clone Repository ---\n",
        "if not already_installed:\n",
        "    print(\"\\nüì• Step 3: Cloning repository...\")\n",
        "    if REPO_DIR.exists():\n",
        "        shutil.rmtree(REPO_DIR)\n",
        "\n",
        "    os.chdir(DRIVE_BASE)\n",
        "    subprocess.run([\n",
        "        \"git\", \"clone\",\n",
        "        \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"\n",
        "    ], check=True)\n",
        "    print(\"   ‚úÖ Repository cloned\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Step 3: Using existing installation\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- STEP 4: Link User Data ---\n",
        "print(\"\\nüîó Step 4: Setting up chat persistence...\")\n",
        "\n",
        "local_user_data = REPO_DIR / \"user_data\"\n",
        "\n",
        "if local_user_data.exists() and not local_user_data.is_symlink():\n",
        "    print(\"   üì¶ Backing up user_data...\")\n",
        "    for item in local_user_data.rglob(\"*\"):\n",
        "        if item.is_file():\n",
        "            rel_path = item.relative_to(local_user_data)\n",
        "            dest = USER_DATA_DIR / rel_path\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(item, dest)\n",
        "    shutil.rmtree(local_user_data)\n",
        "\n",
        "if not local_user_data.exists():\n",
        "    local_user_data.symlink_to(USER_DATA_DIR)\n",
        "    print(\"   ‚úÖ Linked to Google Drive\")\n",
        "\n",
        "(USER_DATA_DIR / \"logs\" / \"chat\").mkdir(parents=True, exist_ok=True)\n",
        "(USER_DATA_DIR / \"logs\" / \"instruct\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- STEP 5: Environment ---\n",
        "print(\"\\nüîß Step 5: Configuring environment...\")\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
        "print(\"   ‚úÖ Environment configured\")\n",
        "\n",
        "# --- STEP 6: Install Dependencies ---\n",
        "print(\"\\nüì¶ Step 6: Installing dependencies...\")\n",
        "\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"],\n",
        "               check=False, stdout=subprocess.DEVNULL)\n",
        "\n",
        "req_files = [\n",
        "    \"requirements_cpu_only.txt\",\n",
        "    \"requirements.txt\",\n",
        "]\n",
        "\n",
        "installed = False\n",
        "for req_file in req_files:\n",
        "    if Path(req_file).exists():\n",
        "        print(f\"   Installing from {req_file}...\")\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                \"-r\", req_file, \"-q\"\n",
        "            ], check=True, timeout=300, capture_output=True, text=True)\n",
        "            installed = True\n",
        "            print(f\"   ‚úÖ Installed from {req_file}\")\n",
        "            break\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Error: {e.stderr[:200]}\")\n",
        "            continue\n",
        "\n",
        "if not installed:\n",
        "    print(\"   Installing core packages...\")\n",
        "    core = [\n",
        "        \"torch\", \"transformers\", \"gradio\",\n",
        "        \"accelerate\", \"markdown\", \"numpy\", \"Pillow\",\n",
        "        \"pyyaml\", \"requests\", \"safetensors\",\n",
        "        \"sentencepiece\", \"tqdm\", \"protobuf\", \"llama-cpp-python\"\n",
        "    ]\n",
        "    for pkg in core:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"],\n",
        "                      check=False, stdout=subprocess.DEVNULL)\n",
        "\n",
        "print(\"   ‚úÖ Dependencies installed\")\n",
        "\n",
        "# --- STEP 7: Fix UI ---\n",
        "print(\"\\nüîß Step 7: Patching UI...\")\n",
        "\n",
        "ui_file = Path(\"modules/ui.py\")\n",
        "if ui_file.exists():\n",
        "    with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    if 'button_shadow_hover' in content and 'if False:  # Disabled' not in content:\n",
        "        content = content.replace(\n",
        "            'if not shared.args.old_colors:',\n",
        "            'if False:  # Disabled for Colab'\n",
        "        )\n",
        "        with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        print(\"   ‚úÖ UI patched\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ UI compatible\")\n",
        "\n",
        "# --- STEP 8: Download Model ---\n",
        "print(\"\\n‚¨áÔ∏è  Step 8: Checking model...\")\n",
        "\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "if MODEL_FILE.exists():\n",
        "    print(f\"   ‚úÖ Model exists ({MODEL_FILE.stat().st_size / (1024**3):.2f} GB)\")\n",
        "else:\n",
        "    print(\"   üì• Downloading model...\")\n",
        "    model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"\n",
        "    subprocess.run([\n",
        "        \"wget\", \"-q\", \"--show-progress\",\n",
        "        \"-O\", str(MODEL_FILE),\n",
        "        model_url\n",
        "    ], check=True)\n",
        "    print(\"   ‚úÖ Model downloaded\")\n",
        "\n",
        "# --- STEP 9: Link Models ---\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "\n",
        "# --- STEP 10: Interactive Configuration ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚öôÔ∏è  CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get hardware mode\n",
        "print(\"\\nüñ•Ô∏è  Select Hardware Mode:\")\n",
        "print(\"1. CPU (Slower, works on any Colab)\")\n",
        "print(\"2. GPU (Faster, requires GPU runtime)\")\n",
        "print(\"3. Auto-detect\")\n",
        "\n",
        "while True:\n",
        "    hardware_choice = input(\"\\nEnter choice (1-3): \").strip()\n",
        "    if hardware_choice in ['1', '2', '3']:\n",
        "        break\n",
        "    print(\"‚ùå Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "if hardware_choice == '1':\n",
        "    use_cpu = True\n",
        "    print(\"‚úÖ Using CPU mode\")\n",
        "elif hardware_choice == '2':\n",
        "    use_cpu = False\n",
        "    print(\"‚úÖ Using GPU mode\")\n",
        "else:\n",
        "    # Auto-detect\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True)\n",
        "        use_cpu = result.returncode != 0\n",
        "        if use_cpu:\n",
        "            print(\"‚ÑπÔ∏è  No GPU detected - using CPU\")\n",
        "        else:\n",
        "            print(\"‚úÖ GPU detected - using GPU\")\n",
        "    except:\n",
        "        use_cpu = True\n",
        "        print(\"‚ÑπÔ∏è  Auto-detection failed - using CPU\")\n",
        "\n",
        "# Get interface mode\n",
        "print(\"\\nüí¨ Select Interface Mode:\")\n",
        "print(\"1. Chat (Conversational AI)\")\n",
        "print(\"2. Instruct (Command-based)\")\n",
        "print(\"3. Default (Chat)\")\n",
        "\n",
        "while True:\n",
        "    mode_choice = input(\"\\nEnter choice (1-3): \").strip()\n",
        "    if mode_choice in ['1', '2', '3']:\n",
        "        break\n",
        "    print(\"‚ùå Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "if mode_choice == '2':\n",
        "    interface_mode = '--instruct'\n",
        "    print(\"‚úÖ Using Instruct mode\")\n",
        "else:\n",
        "    interface_mode = '--chat'\n",
        "    print(\"‚úÖ Using Chat mode\")\n",
        "\n",
        "# Additional options\n",
        "print(\"\\nüîß Additional Options:\")\n",
        "print(\"1. Enable API (allows external access)\")\n",
        "print(\"2. Verbose logging (more detailed output)\")\n",
        "print(\"3. Both\")\n",
        "print(\"4. None\")\n",
        "\n",
        "while True:\n",
        "    options_choice = input(\"\\nEnter choice (1-4): \").strip()\n",
        "    if options_choice in ['1', '2', '3', '4']:\n",
        "        break\n",
        "    print(\"‚ùå Invalid choice. Please enter 1, 2, 3, or 4.\")\n",
        "\n",
        "extra_args = []\n",
        "if options_choice in ['1', '3']:\n",
        "    extra_args.extend(['--api', '--public-api'])\n",
        "    print(\"‚úÖ API enabled\")\n",
        "if options_choice in ['2', '3']:\n",
        "    extra_args.append('--verbose')\n",
        "    print(\"‚úÖ Verbose logging enabled\")\n",
        "\n",
        "# --- STEP 11: Build Launch Command ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ LAUNCHING MY-AI-GIZMO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Build command\n",
        "launch_cmd = [sys.executable, \"server.py\"]\n",
        "\n",
        "# Add hardware mode\n",
        "if use_cpu:\n",
        "    launch_cmd.append(\"--cpu\")\n",
        "else:\n",
        "    launch_cmd.append(\"--auto-devices\")\n",
        "\n",
        "# Add interface mode\n",
        "launch_cmd.append(interface_mode)\n",
        "\n",
        "# Add standard args\n",
        "launch_cmd.extend([\n",
        "    \"--listen\",\n",
        "    \"--share\",\n",
        "    \"--model\", str(MODEL_FILE)\n",
        "])\n",
        "\n",
        "# Add extra args\n",
        "launch_cmd.extend(extra_args)\n",
        "\n",
        "print(f\"\\nüìù Launch Configuration:\")\n",
        "print(f\"   Hardware: {'CPU' if use_cpu else 'GPU'}\")\n",
        "print(f\"   Interface: {'Chat' if '--chat' in launch_cmd else 'Instruct'}\")\n",
        "print(f\"   API: {'Enabled' if '--api' in launch_cmd else 'Disabled'}\")\n",
        "print(f\"   Verbose: {'Yes' if '--verbose' in launch_cmd else 'No'}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "\n",
        "print(f\"\\nüíæ Chats save to: {USER_DATA_DIR / 'logs'}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚è≥ Starting server (may take 1-2 minutes)...\")\n",
        "print(\"üîó LOOK FOR THE GRADIO LINK BELOW:\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# --- STEP 12: Launch with Error Handling ---\n",
        "try:\n",
        "    # First, check if server.py exists and is valid\n",
        "    if not Path(\"server.py\").exists():\n",
        "        print(\"‚ùå ERROR: server.py not found!\")\n",
        "        print(f\"Current directory: {os.getcwd()}\")\n",
        "        print(\"\\nDirectory contents:\")\n",
        "        subprocess.run([\"ls\", \"-la\"], check=False)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Try to run with captured output for better error messages\n",
        "    print(\"Starting server...\\n\")\n",
        "    process = subprocess.Popen(\n",
        "        launch_cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    # Print output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "        # Check for Gradio link\n",
        "        if 'gradio.live' in line.lower() or 'running on' in line.lower():\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"‚úÖ SERVER RUNNING! Use the link above ‚òùÔ∏è\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n‚èπÔ∏è  Server stopped by user\")\n",
        "    print(f\"üíæ Chats saved in: {USER_DATA_DIR / 'logs'}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\n‚ùå ERROR: {e}\")\n",
        "    print(\"\\nüîç Troubleshooting:\")\n",
        "    print(f\"   ‚Ä¢ Working directory: {os.getcwd()}\")\n",
        "    print(f\"   ‚Ä¢ Model exists: {MODEL_FILE.exists()}\")\n",
        "    print(f\"   ‚Ä¢ Python: {sys.executable}\")\n",
        "\n",
        "    # Try to get more error info\n",
        "    print(\"\\nüìã Trying to run with error details...\")\n",
        "    result = subprocess.run(\n",
        "        launch_cmd,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    print(\"\\nSTDOUT:\")\n",
        "    print(result.stdout)\n",
        "    print(\"\\nSTDERR:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "    print(\"\\nüí° Common fixes:\")\n",
        "    print(\"   1. Restart runtime and try again\")\n",
        "    print(\"   2. Check if requirements installed correctly\")\n",
        "    print(\"   3. Try running: !python server.py --help\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}