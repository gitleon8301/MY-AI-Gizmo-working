{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# MY-AI-Gizmo ‚Ä¢ LAUNCHER WITH VERBOSE INSTALLATION (FIXED)\n",
        "# - avoids long silent llama.cpp builds when possible\n",
        "# - skips re-install if environment already exists\n",
        "# - provides heartbeat when install is silent\n",
        "# - more robust download + symlink fallback\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except Exception:\n",
        "    drive = None\n",
        "\n",
        "# ========== Configuration ==========\n",
        "REPO_ZIP = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "HEARTBEAT_INTERVAL = 30  # seconds\n",
        "# ===================================\n",
        "\n",
        "def sh(cmd, check=False, cwd=None, env=None):\n",
        "    return subprocess.run(cmd, shell=True, capture_output=True, text=True, check=check, cwd=cwd, env=env)\n",
        "\n",
        "def sh_live(cmd, cwd=None, env=None):\n",
        "    \"\"\"\n",
        "    Run command and stream stdout/stderr in real-time.\n",
        "    Provides a heartbeat message if no output for HEARTBEAT_INTERVAL.\n",
        "    Returns process returncode.\n",
        "    \"\"\"\n",
        "    proc = subprocess.Popen(\n",
        "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "        text=True, bufsize=1, cwd=cwd, env=env\n",
        "    )\n",
        "\n",
        "    last_output = time.time()\n",
        "    stop_flag = threading.Event()\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop_flag.wait(HEARTBEAT_INTERVAL):\n",
        "            # If more than HEARTBEAT_INTERVAL since last output, print heartbeat\n",
        "            if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                print(f\"‚è≥ still working... (no new output for ~{HEARTBEAT_INTERVAL}s)\")\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            print(line, end='')\n",
        "            last_output = time.time()\n",
        "    except Exception:\n",
        "        pass\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop_flag.set()\n",
        "        hb.join(timeout=1)\n",
        "\n",
        "    return proc.returncode\n",
        "\n",
        "# Environment setup\n",
        "print(\"üîß Setting up environment...\")\n",
        "os.environ.pop(\"MPLBACKEND\", None)\n",
        "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
        "print(\"‚úì Environment ready\\n\")\n",
        "\n",
        "# CUDA helper\n",
        "def fix_cuda_library_path():\n",
        "    print(\"üîß Fixing CUDA library paths...\")\n",
        "    cuda_paths = [\n",
        "        '/usr/local/cuda/lib64',\n",
        "        '/usr/local/cuda-12/lib64',\n",
        "        '/usr/lib/x86_64-linux-gnu',\n",
        "        '/usr/local/nvidia/lib64',\n",
        "    ]\n",
        "    valid_paths = []\n",
        "    for path in cuda_paths:\n",
        "        p = Path(path)\n",
        "        if p.exists() and any(p.glob('libcuda.so*')):\n",
        "            valid_paths.append(path)\n",
        "            print(f\"  ‚úì {path}\")\n",
        "    if valid_paths:\n",
        "        os.environ['LD_LIBRARY_PATH'] = ':'.join(valid_paths)\n",
        "        print(\"  ‚úì Set LD_LIBRARY_PATH\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# File helpers\n",
        "def _ensure_drive_path(drive_path: Path, is_settings_file=False):\n",
        "    if drive_path.suffix:\n",
        "        drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        if is_settings_file and not drive_path.exists():\n",
        "            drive_path.touch(exist_ok=True)\n",
        "    else:\n",
        "        drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _remove_path(path: Path):\n",
        "    try:\n",
        "        if path.is_symlink() or path.exists():\n",
        "            if path.is_symlink():\n",
        "                path.unlink()\n",
        "            elif path.is_dir():\n",
        "                shutil.rmtree(path)\n",
        "            else:\n",
        "                path.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _create_symlink_or_fallback(src: Path, dest: Path):\n",
        "    try:\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "        if dest.exists() or dest.is_symlink():\n",
        "            _remove_path(dest)\n",
        "        os.symlink(str(src), str(dest), target_is_directory=src.is_dir())\n",
        "        return True\n",
        "    except Exception:\n",
        "        # fallback to copy\n",
        "        try:\n",
        "            if src.is_dir():\n",
        "                if dest.exists():\n",
        "                    _remove_path(dest)\n",
        "                shutil.copytree(src, dest)\n",
        "            else:\n",
        "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(src, dest)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "def cleanup_broken_files(drive_root: Path):\n",
        "    print(\"\\nüßπ Cleaning broken files...\")\n",
        "    models_dir = drive_root / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        print(\"  No models directory yet\")\n",
        "        return\n",
        "    extensions = [\"*.gguf\", \"*.safetensors\", \"*.bin\", \"*.pth\", \"*.pt\"]\n",
        "    broken = []\n",
        "    for ext in extensions:\n",
        "        for f in models_dir.rglob(ext):\n",
        "            try:\n",
        "                if f.stat().st_size < (100 * 1024):\n",
        "                    broken.append(f)\n",
        "            except Exception:\n",
        "                pass\n",
        "    if broken:\n",
        "        print(f\"  Found {len(broken)} broken files - deleting...\")\n",
        "        for f in broken:\n",
        "            try:\n",
        "                f.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "        print(\"  ‚úì Cleaned\")\n",
        "    else:\n",
        "        print(\"  ‚úì No broken files\")\n",
        "\n",
        "# MAIN\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ MY-AI-Gizmo Setup (Verbose Mode)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Mount Drive\n",
        "print(\"\\nüìÅ Step 1/6: Mounting Drive...\")\n",
        "if drive:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"‚úì Mounted\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  {e}\")\n",
        "else:\n",
        "    print(\"  Note: google.colab.drive not available in this environment\")\n",
        "\n",
        "# Cleanup broken models\n",
        "cleanup_broken_files(DRIVE_ROOT)\n",
        "\n",
        "# Step 2: Create folders\n",
        "print(\"\\nüíæ Step 2/6: Creating folders...\")\n",
        "folders = [\"models\", \"loras\", \"training\", \"characters\", \"presets\", \"prompts\",\n",
        "           \"settings\", \"chat-history\", \"instruct-history\", \"outputs\", \"images\",\n",
        "           \"logs\", \"cache\", \"extensions\", \"softprompts\"]\n",
        "for f in folders:\n",
        "    (DRIVE_ROOT / f).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úì {len(folders)} folders\")\n",
        "\n",
        "# Step 3: Download repo (if needed)\n",
        "print(\"\\nüì• Step 3/6: Repository...\")\n",
        "if not WORK_DIR.exists():\n",
        "    # try download with wget or curl, with retries\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try:\n",
        "        tmp_zip.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"  Downloading...\")\n",
        "    got = False\n",
        "    # try wget\n",
        "    try:\n",
        "        r = sh(f\"wget -q -O {tmp_zip} {REPO_ZIP}\")\n",
        "        if tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            got = True\n",
        "    except Exception:\n",
        "        got = False\n",
        "    # try curl fallback\n",
        "    if not got:\n",
        "        try:\n",
        "            r = sh(f\"curl -s -L -o {tmp_zip} {REPO_ZIP}\")\n",
        "            if tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "                got = True\n",
        "        except Exception:\n",
        "            got = False\n",
        "    if not got:\n",
        "        print(\"‚ö†Ô∏è  Download failed. Please check network or REPO_ZIP URL.\")\n",
        "    else:\n",
        "        try:\n",
        "            sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "            found = next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\"), None)\n",
        "            if found:\n",
        "                found.rename(WORK_DIR)\n",
        "                print(\"‚úì Downloaded and extracted\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  Extraction succeeded but expected folder not found\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Unzip failed: {e}\")\n",
        "else:\n",
        "    print(\"‚úì Exists\")\n",
        "\n",
        "if WORK_DIR.exists():\n",
        "    os.chdir(WORK_DIR)\n",
        "\n",
        "# Step 4: Symlinks\n",
        "print(\"\\nüîó Step 4/6: Linking...\")\n",
        "links_map = [\n",
        "    (\"models\", \"models\", False),\n",
        "    (\"loras\", \"loras\", False),\n",
        "    (\"user_data/characters\", \"characters\", False),\n",
        "    (\"user_data/presets\", \"presets\", False),\n",
        "    (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "    (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "    (\"user_data/chat\", \"chat-history\", False),\n",
        "    (\"outputs\", \"outputs\", False),\n",
        "]\n",
        "\n",
        "for local, drive_folder, is_settings in links_map:\n",
        "    drive_path = DRIVE_ROOT / drive_folder\n",
        "    _ensure_drive_path(drive_path, is_settings_file=is_settings)\n",
        "    local_path = WORK_DIR / local\n",
        "    if local_path.exists() or local_path.is_symlink():\n",
        "        _remove_path(local_path)\n",
        "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    ok = _create_symlink_or_fallback(drive_path, local_path)\n",
        "    if not ok:\n",
        "        print(f\"  ‚ö†Ô∏è  Failed to link {local} -> {drive_path}\")\n",
        "print(\"‚úì Linked\")\n",
        "\n",
        "# Step 5: Settings\n",
        "print(\"\\n‚öôÔ∏è  Step 5/6: Settings...\")\n",
        "drive_settings = DRIVE_ROOT / \"settings\" / \"settings.yaml\"\n",
        "local_settings = WORK_DIR / \"user_data\" / \"settings.yaml\"\n",
        "local_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "if local_settings.is_symlink():\n",
        "    try:\n",
        "        local_settings.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "if drive_settings.exists() and drive_settings.stat().st_size > 0:\n",
        "    try:\n",
        "        shutil.copy2(drive_settings, local_settings)\n",
        "        print(\"‚úì Copied from Drive\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Copy failed: {e}\")\n",
        "else:\n",
        "    try:\n",
        "        local_settings.write_text(\"# minimal\\nlisten: true\\nshare: true\\n\")\n",
        "        print(\"‚úì Created\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not create settings: {e}\")\n",
        "\n",
        "# ====== PREPARE FOR INSTALL: avoid long silent llama.cpp builds ======\n",
        "# Strategy:\n",
        "#  - If a local prebuilt environment exists, skip full install.\n",
        "#  - If repositories/llama.cpp exists, rename it to avoid automatic rebuild (safe fallback).\n",
        "#  - Set environment flags to request a fast install.\n",
        "print(\"\\nüîç Preparing fast-install safeguards...\")\n",
        "env_marker = WORK_DIR / \"installer_files/env/bin/python\"\n",
        "if env_marker.exists():\n",
        "    print(\"‚ö° Environment already exists ‚Äî installer will be skipped by default.\")\n",
        "else:\n",
        "    # detect and disable llama.cpp auto-build by renaming the folder if present\n",
        "    llama_dir = WORK_DIR / \"repositories\" / \"llama.cpp\"\n",
        "    if llama_dir.exists() and llama_dir.is_dir():\n",
        "        disabled = llama_dir.with_name(llama_dir.name + \".disabled\")\n",
        "        try:\n",
        "            if disabled.exists():\n",
        "                shutil.rmtree(disabled, ignore_errors=True)\n",
        "            llama_dir.rename(disabled)\n",
        "            print(\"‚ö° Renamed repositories/llama.cpp to prevent automatic rebuild (fast mode).\")\n",
        "        except Exception:\n",
        "            print(\"  ‚ö†Ô∏è Could not rename llama.cpp; will attempt env flags instead.\")\n",
        "\n",
        "# Step 6: Install (LIVE)\n",
        "print(\"\\nüì¶ Step 6/6: Installing dependencies...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä LIVE INSTALLATION OUTPUT (showing progress)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nThis may take several minutes on first run; if it looks silent, a heartbeat will be printed every 30s.\\n\")\n",
        "print(\"Watch for these stages:\")\n",
        "print(\"  1Ô∏è‚É£  Creating environment\")\n",
        "print(\"  2Ô∏è‚É£  Installing PyTorch (~2GB)\")\n",
        "print(\"  3Ô∏è‚É£  Installing packages\")\n",
        "print(\"  4Ô∏è‚É£  (optional) Compiling llama-cpp (skipped in fast mode)\")\n",
        "print(\"  5Ô∏è‚É£  Finishing up\")\n",
        "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Ensure start script executable\n",
        "try:\n",
        "    sh(\"chmod +x start_linux.sh\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# If env exists, skip installation\n",
        "start_time = time.time()\n",
        "returncode = 0\n",
        "if env_marker.exists():\n",
        "    print(\"‚ö° Skipping install; virtualenv already present.\")\n",
        "else:\n",
        "    install_env = os.environ.copy()\n",
        "    # Suggested flags; start_linux.sh may read these. They reduce long rebuilds.\n",
        "    install_env.update({\n",
        "        \"GPU_CHOICE\": \"A\",\n",
        "        \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "        \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "        \"SKIP_LLAMACPP_BUILD\": \"TRUE\",   # best-effort; depends on start_linux.sh\n",
        "        \"SKIP_TORCH_TEST\": \"TRUE\",       # best-effort\n",
        "        \"FORCE_CUDA\": \"FALSE\",           # avoid forcing heavy cuda builds\n",
        "        # keep MPLBACKEND set\n",
        "        \"MPLBACKEND\": \"Agg\",\n",
        "    })\n",
        "\n",
        "    # Run installer with live output; cwd = WORK_DIR\n",
        "    cmd = \"bash start_linux.sh\"\n",
        "    try:\n",
        "        returncode = sh_live(cmd, cwd=str(WORK_DIR), env=install_env)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Installer execution error: {e}\")\n",
        "        returncode = 1\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if returncode == 0:\n",
        "    print(f\"‚úÖ Installation finished ({elapsed:.1f}s)\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Installation completed with warnings or errors (exit code {returncode}, took {elapsed:.1f}s)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Attempt to restore llama.cpp folder name so user can opt into building later\n",
        "renamed = WORK_DIR / \"repositories\" / \"llama.cpp.disabled\"\n",
        "if renamed.exists():\n",
        "    try:\n",
        "        restored = renamed.with_name(\"llama.cpp\")\n",
        "        if not restored.exists():\n",
        "            renamed.rename(restored)\n",
        "            print(\"‚ÑπÔ∏è  Restored repositories/llama.cpp (it won't be auto-built now unless start script forces it).\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# CUDA fix\n",
        "print(\"\\nüîß Setting up CUDA (best-effort)...\")\n",
        "fix_cuda_library_path()\n",
        "\n",
        "# LAUNCH\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üåê LAUNCHING UI\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# kill stray python processes that might block port binding\n",
        "try:\n",
        "    sh(\"pkill -9 -f python\")\n",
        "except Exception:\n",
        "    pass\n",
        "time.sleep(1.5)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MPLBACKEND\"] = \"Agg\"\n",
        "env.pop(\"PYTHONPATH\", None)\n",
        "\n",
        "python_exe = str(WORK_DIR / \"installer_files/env/bin/python\")\n",
        "if not Path(python_exe).exists():\n",
        "    python_exe = \"python3\"\n",
        "\n",
        "cmd = f\"{python_exe} -u server.py --share --listen\"\n",
        "proc = subprocess.Popen(\n",
        "    cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "    text=True, bufsize=1, env=env, cwd=str(WORK_DIR)\n",
        ")\n",
        "\n",
        "shown = False\n",
        "for line in proc.stdout:\n",
        "    print(line, end=\"\")\n",
        "    # detect Gradio public URL\n",
        "    if not shown and (\"Running on public URL:\" in line or \"Running on local URL:\" in line or \"Running on \" in line):\n",
        "        m = re.search(r\"(https?://[a-z0-9\\-\\._]+\\.gradio\\.live\\S*)\", line)\n",
        "        if not m:\n",
        "            m = re.search(r\"(https?://\\S+:\\d+)\", line)\n",
        "        if m:\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(\"‚ú® SUCCESS! ‚ú®\")\n",
        "            print(\"=\" * 70)\n",
        "            print(f\"\\nüåç {m.group(1)}\")\n",
        "            print(\"\\nüéØ Model tab ‚Üí Select model ‚Üí Load ‚Üí Chat!\")\n",
        "            print(\"=\" * 70 + \"\\n\")\n",
        "            shown = True\n",
        "\n",
        "print(\"\\n‚úì Done\")\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}