{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLAMA MODELS - NO TOKEN - SIMPLE DOWNLOAD\n",
        "import os, shutil, subprocess, sys, time\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- LLAMA MODEL CHOICES ----------\n",
        "\n",
        "# ü¶ô OPTION 1: Llama 3.2 3B (NEWEST, SMALL, FAST)\n",
        "MODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\n",
        "SPECIFIC_FILE = \"Llama-3.2-3B-Instruct-Q5_K_M.gguf\"\n",
        "# Size: 2.6GB | Speed: 3-5 tok/s | RAM: 5GB | NO TOKEN NEEDED\n",
        "\n",
        "# ü¶ô OPTION 2: Llama 3.1 8B (BETTER QUALITY, STILL FAST)\n",
        "# MODEL_REPO = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
        "# SPECIFIC_FILE = \"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
        "# Size: 4.9GB | Speed: 2-3 tok/s | RAM: 7GB | NO TOKEN NEEDED\n",
        "\n",
        "# ü¶ô OPTION 3: Llama 2 7B Chat (CLASSIC, PROVEN)\n",
        "# MODEL_REPO = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
        "# SPECIFIC_FILE = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "# Size: 4.1GB | Speed: 2-3 tok/s | RAM: 7GB | NO TOKEN NEEDED\n",
        "\n",
        "# ü¶ô OPTION 4: Llama 3.1 70B (BEST QUALITY - 42GB!)\n",
        "# MODEL_REPO = \"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\"\n",
        "# SPECIFIC_FILE = \"Meta-Llama-3.1-70B-Instruct-Q3_K_M.gguf\"\n",
        "# Size: 29GB | Speed: 0.3 tok/s | RAM: 35GB | NO TOKEN NEEDED\n",
        "# ‚ö†Ô∏è TOO BIG FOR COLAB FREE - Need Colab Pro or local machine\n",
        "\n",
        "# All these are COMMUNITY UPLOADS - NO TOKEN REQUIRED!\n",
        "# ------------------------------------------\n",
        "\n",
        "GIT_URL = \"https://github.com/gitleon8301/MY-AI-Gizmo-working\"\n",
        "REPO_DIR_NAME = \"text-generation-webui\"\n",
        "\n",
        "# Download method: try both HF and wget\n",
        "USE_HF = True  # Set to False to skip HuggingFace library entirely\n",
        "\n",
        "DOWNLOAD_LOG = Path(\"download.log\")\n",
        "START_LOG = Path(\"webui.log\")\n",
        "\n",
        "cwd = Path.cwd()\n",
        "repo_dir = cwd / REPO_DIR_NAME\n",
        "models_dir = cwd / \"models\" / MODEL_REPO.replace(\"/\", \"_\")\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Force CPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "def run(cmd, cwd=None, env=None, logfile=None, check=False):\n",
        "    print(f\"\\n>>> {cmd}\")\n",
        "    with subprocess.Popen(cmd, shell=True, cwd=cwd, env=env or os.environ,\n",
        "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True) as p:\n",
        "        for line in p.stdout:\n",
        "            if logfile:\n",
        "                with open(logfile, \"a\") as f:\n",
        "                    f.write(line)\n",
        "            print(line, end=\"\")\n",
        "        p.wait()\n",
        "        if check and p.returncode != 0:\n",
        "            raise subprocess.CalledProcessError(p.returncode, cmd)\n",
        "        return p.returncode\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"ü¶ô MODEL: {MODEL_REPO}\")\n",
        "print(f\"üì¶ FILE: {SPECIFIC_FILE}\")\n",
        "print(\"‚úÖ NO TOKEN NEEDED - Community upload!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1) Clone repo\n",
        "if not repo_dir.exists():\n",
        "    print(\"\\nüì¶ Cloning web-UI...\")\n",
        "    run(f\"git clone {GIT_URL} {REPO_DIR_NAME}\", check=True)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# 2) Install\n",
        "print(\"\\nüîß Installing (CPU mode)...\")\n",
        "run('GPU_CHOICE=N LAUNCH_AFTER_INSTALL=FALSE INSTALL_EXTENSIONS=FALSE ./start_linux.sh',\n",
        "    check=False, logfile=START_LOG)\n",
        "\n",
        "# 3) Download model\n",
        "target_file = models_dir / SPECIFIC_FILE\n",
        "download_success = False\n",
        "\n",
        "if target_file.exists() and target_file.stat().st_size > 1_000_000:\n",
        "    print(f\"\\n‚úì Model already exists: {target_file}\")\n",
        "    download_success = True\n",
        "\n",
        "# METHOD 1: Try HuggingFace Hub (simple, resumable)\n",
        "if not download_success and USE_HF:\n",
        "    print(\"\\n‚¨áÔ∏è  Method 1: Downloading via HuggingFace Hub...\")\n",
        "    print(\"üì• Installing huggingface_hub...\")\n",
        "    run(\"pip install -q huggingface_hub\", logfile=DOWNLOAD_LOG)\n",
        "\n",
        "    try:\n",
        "        from huggingface_hub import hf_hub_download\n",
        "        print(f\"üì• Downloading {SPECIFIC_FILE}...\")\n",
        "        downloaded = hf_hub_download(\n",
        "            repo_id=MODEL_REPO,\n",
        "            filename=SPECIFIC_FILE,\n",
        "            local_dir=str(models_dir),\n",
        "            resume_download=True\n",
        "            # NO TOKEN PARAMETER - not needed!\n",
        "        )\n",
        "        print(f\"‚úì Downloaded: {downloaded}\")\n",
        "        download_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå HF download failed: {e}\")\n",
        "        print(\"Trying alternative method...\")\n",
        "\n",
        "# METHOD 2: Direct wget (no HF dependency)\n",
        "if not download_success:\n",
        "    print(\"\\n‚¨áÔ∏è  Method 2: Direct download via wget (no HuggingFace needed)...\")\n",
        "    file_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{SPECIFIC_FILE}\"\n",
        "    print(f\"üì• URL: {file_url}\")\n",
        "    print(f\"üìÅ Saving to: {target_file}\")\n",
        "\n",
        "    try:\n",
        "        # wget -c allows resuming if interrupted\n",
        "        ret = run(f\"wget -c '{file_url}' -O '{target_file}'\", logfile=DOWNLOAD_LOG, check=False)\n",
        "        if ret == 0 and target_file.exists() and target_file.stat().st_size > 1_000_000:\n",
        "            print(f\"‚úì Downloaded via wget: {target_file}\")\n",
        "            download_success = True\n",
        "        else:\n",
        "            print(f\"‚ùå Wget failed (exit code {ret})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Wget error: {e}\")\n",
        "\n",
        "# METHOD 3: Manual instructions\n",
        "if not download_success:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ùå AUTOMATIC DOWNLOAD FAILED\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nüì• MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
        "    print(f\"\\n1. Open this URL in your browser:\")\n",
        "    print(f\"   https://huggingface.co/{MODEL_REPO}/blob/main/{SPECIFIC_FILE}\")\n",
        "    print(f\"\\n2. Click the 'download' button\")\n",
        "    print(f\"\\n3. In Colab, click the folder icon (üìÅ) in left sidebar\")\n",
        "    print(f\"\\n4. Navigate to: {models_dir}\")\n",
        "    print(f\"\\n5. Click upload button and select the downloaded file\")\n",
        "    print(f\"\\n6. Re-run this cell after upload completes\")\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    raise SystemExit(1)\n",
        "\n",
        "# 4) Verify\n",
        "gguf_files = list(models_dir.glob(\"*.gguf\"))\n",
        "if not gguf_files:\n",
        "    print(f\"\\n‚ùå No .gguf files in {models_dir}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "print(f\"\\n‚úì Model ready!\")\n",
        "run(f\"ls -lh {models_dir}\")\n",
        "\n",
        "# 5) Start\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ STARTING WEB UI\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_name = MODEL_REPO.replace('/', '_')\n",
        "start_cmd = (\n",
        "    f\"python server.py \"\n",
        "    f\"--cpu \"\n",
        "    f\"--share \"\n",
        "    f\"--model-dir {models_dir.parent} \"\n",
        "    f\"--model {model_name} \"\n",
        "    f\"--n-gpu-layers 0 \"\n",
        "    f\"--threads {os.cpu_count() or 4}\"\n",
        ")\n",
        "\n",
        "print(\"\\nüåê Look for the Gradio public URL below...\")\n",
        "print(\"üìù Logs:\", START_LOG)\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    run(start_cmd, logfile=START_LOG)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚èπÔ∏è  Stopped\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    print(\"\\nüìã Last 50 log lines:\")\n",
        "    run(f\"tail -n 50 {START_LOG}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}