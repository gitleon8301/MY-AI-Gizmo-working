{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PASTE THIS IN COLAB AND RUN - NO TOKEN NEEDED!\n",
        "import os, shutil, subprocess, sys, time\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- USER-EDITABLE ----------\n",
        "GIT_URL = \"https://github.com/gitleon8301/MY-AI-Gizmo-working\"\n",
        "REPO_DIR_NAME = \"text-generation-webui\"\n",
        "\n",
        "# ‚úÖ OPTION 1: Mistral 7B (recommended - good quality, no token needed)\n",
        "MODEL_REPO = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "SPECIFIC_FILE = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "\n",
        "# ‚úÖ OPTION 2: Phi-3 Mini (smaller, faster, still good)\n",
        "# MODEL_REPO = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
        "# SPECIFIC_FILE = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
        "\n",
        "# ‚úÖ OPTION 3: TinyLlama (very small, very fast, lower quality)\n",
        "# MODEL_REPO = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
        "# SPECIFIC_FILE = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "\n",
        "# NO TOKEN NEEDED - these models are fully open!\n",
        "# ------------------------------------\n",
        "\n",
        "# Logs\n",
        "DOWNLOAD_LOG = Path(\"download.log\")\n",
        "START_LOG = Path(\"webui.log\")\n",
        "\n",
        "# Setup paths\n",
        "cwd = Path.cwd()\n",
        "repo_dir = cwd / REPO_DIR_NAME\n",
        "models_dir = cwd / \"models\" / MODEL_REPO.replace(\"/\", \"_\")\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Force CPU only\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "os.environ.pop('MPLBACKEND', None)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Hide GPUs\n",
        "\n",
        "def run(cmd, cwd=None, env=None, logfile=None, check=False):\n",
        "    print(f\"\\n>>> RUN: {cmd}\")\n",
        "    with subprocess.Popen(cmd, shell=True, cwd=cwd, env=env or os.environ,\n",
        "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True) as p:\n",
        "        out_lines = []\n",
        "        for line in p.stdout:\n",
        "            out_lines.append(line)\n",
        "            if logfile:\n",
        "                with open(logfile, \"a\") as f:\n",
        "                    f.write(line)\n",
        "            print(line, end=\"\")\n",
        "        p.wait()\n",
        "        if check and p.returncode != 0:\n",
        "            raise subprocess.CalledProcessError(p.returncode, cmd)\n",
        "        return p.returncode, \"\".join(out_lines)\n",
        "\n",
        "# 1) Clone the repo if missing\n",
        "if not repo_dir.exists():\n",
        "    print(\"üì¶ Cloning web-UI repo...\")\n",
        "    run(f\"git clone {GIT_URL} {REPO_DIR_NAME}\", check=True)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# 2) Install dependencies (skip auto-launch)\n",
        "print(\"\\nüîß Running installation (CPU mode)...\")\n",
        "run('GPU_CHOICE=N LAUNCH_AFTER_INSTALL=FALSE INSTALL_EXTENSIONS=FALSE ./start_linux.sh',\n",
        "    check=False, logfile=START_LOG)\n",
        "\n",
        "# 3) Install huggingface_hub\n",
        "print(\"\\nüìö Installing huggingface_hub...\")\n",
        "run(\"pip install -q huggingface_hub\", logfile=DOWNLOAD_LOG)\n",
        "\n",
        "# 4) Download model - NO TOKEN REQUIRED!\n",
        "print(f\"\\n‚¨áÔ∏è  Downloading {SPECIFIC_FILE} from {MODEL_REPO}...\")\n",
        "print(\"‚úÖ This model is fully open - no authentication needed!\")\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    target_file = models_dir / SPECIFIC_FILE\n",
        "\n",
        "    if target_file.exists() and target_file.stat().st_size > 1_000_000:\n",
        "        print(f\"‚úì Model file already exists: {target_file}\")\n",
        "    else:\n",
        "        print(f\"Downloading to: {models_dir}\")\n",
        "        downloaded_path = hf_hub_download(\n",
        "            repo_id=MODEL_REPO,\n",
        "            filename=SPECIFIC_FILE,\n",
        "            local_dir=str(models_dir),\n",
        "            resume_download=True  # Resume if interrupted\n",
        "            # NO token parameter - not needed!\n",
        "        )\n",
        "        print(f\"‚úì Downloaded: {downloaded_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Download failed: {repr(e)}\")\n",
        "    print(\"\\nTrying alternative download method...\")\n",
        "\n",
        "    # Fallback: use wget\n",
        "    try:\n",
        "        file_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{SPECIFIC_FILE}\"\n",
        "        target = models_dir / SPECIFIC_FILE\n",
        "        run(f\"wget -c '{file_url}' -O '{target}'\", logfile=DOWNLOAD_LOG, check=True)\n",
        "        print(f\"‚úì Downloaded via wget: {target}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Wget also failed: {repr(e2)}\")\n",
        "        print(\"\\nüîß Manual download option:\")\n",
        "        print(f\"1. Go to: https://huggingface.co/{MODEL_REPO}/tree/main\")\n",
        "        print(f\"2. Download: {SPECIFIC_FILE}\")\n",
        "        print(f\"3. Upload it to Colab into this folder: {models_dir}\")\n",
        "        raise SystemExit(1)\n",
        "\n",
        "# 5) Verify model file exists\n",
        "gguf_files = list(models_dir.glob(\"*.gguf\"))\n",
        "if not gguf_files:\n",
        "    print(f\"\\n‚ùå No .gguf files found in {models_dir}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "print(f\"\\n‚úì Model ready! Files in {models_dir}:\")\n",
        "run(f\"ls -lh {models_dir}\")\n",
        "\n",
        "# 6) Start the web UI (CPU mode with --share)\n",
        "print(\"\\nüöÄ Starting web UI in CPU mode...\")\n",
        "print(\"‚ö†Ô∏è  CPU inference is slow - expect 1-5 tokens/second\")\n",
        "print(\"üìù Logs streaming to\", START_LOG)\n",
        "\n",
        "model_name = MODEL_REPO.replace('/', '_')\n",
        "start_cmd = (\n",
        "    f\"python server.py \"\n",
        "    f\"--cpu \"\n",
        "    f\"--share \"\n",
        "    f\"--model-dir {models_dir.parent} \"\n",
        "    f\"--model {model_name} \"\n",
        "    f\"--n-gpu-layers 0 \"\n",
        "    f\"--threads {os.cpu_count() or 4}\"\n",
        ")\n",
        "\n",
        "print(f\"\\nüåê Look for the Gradio public URL in the output below!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    run(start_cmd, logfile=START_LOG)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n‚èπÔ∏è  Stopped by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Web UI failed: {repr(e)}\")\n",
        "    print(\"\\nLast 50 lines of log:\")\n",
        "    run(f\"tail -n 50 {START_LOG}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}