{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo ‚Ä¢ LAUNCHER ‚Äî FIXED\n",
        "# - Prevents long silent \"Finishing up\" hangs by skipping llama.cpp build\n",
        "# - Renames llama.cpp repo to avoid automatic rebuild (safe fallback)\n",
        "# - Streams installer output live with heartbeat so it never appears stuck\n",
        "# - Forces matplotlib backend inside server process to Agg to avoid backend errors\n",
        "# - Captures Gradio public URL and writes it to Drive\n",
        "# - Persists logs, settings, models, chat-history to DRIVE_ROOT\n",
        "# Intended for Google Colab / Linux with Drive mounted at /content/drive\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "REPO_ZIP = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "LOG_DIR = DRIVE_ROOT / \"logs\"\n",
        "MPL_CONFIG_DIR = DRIVE_ROOT / \"matplotlib\"\n",
        "HEARTBEAT_INTERVAL = 30  # seconds\n",
        "PUBLIC_URL_FILE = DRIVE_ROOT / \"public_url.txt\"\n",
        "# -----------------------------------\n",
        "\n",
        "def sh(cmd, cwd=None, env=None, check=False):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env, capture_output=True, text=True, check=check)\n",
        "\n",
        "def stream_with_heartbeat(cmd, cwd=None, env=None, logfile_path=None, capture_url_to=None):\n",
        "    \"\"\"\n",
        "    Run command, stream output line-by-line, print heartbeat if silent.\n",
        "    Capture first Gradio/public URL found and optionally save it to capture_url_to.\n",
        "    Returns (returncode, captured_url_or_None).\n",
        "    \"\"\"\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                            cwd=cwd, env=env, text=True, bufsize=1)\n",
        "\n",
        "    last_output = time.time()\n",
        "    stop = threading.Event()\n",
        "    captured_url = None\n",
        "\n",
        "    url_patterns = [\n",
        "        re.compile(r\"(https?://[^\\s)'\\\"]+gradio\\.\\w+[^\\s)'\\\"]*)\", re.IGNORECASE),\n",
        "        re.compile(r\"(https?://[^\\s)'\\\"]+\\.gradio\\.live[^\\s)'\\\"]*)\", re.IGNORECASE),\n",
        "        re.compile(r\"(https?://[^\\s)'\\\"]+:\\d+)\", re.IGNORECASE),\n",
        "        re.compile(r\"(https?://[^\\s)'\\\"]+)\", re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                msg = f\"[heartbeat] still working... (no output for ~{HEARTBEAT_INTERVAL}s)\\n\"\n",
        "                print(msg, end='')\n",
        "                if logfile_path:\n",
        "                    try:\n",
        "                        with open(logfile_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                            f.write(msg)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "\n",
        "    logfile = None\n",
        "    if logfile_path:\n",
        "        try:\n",
        "            logfile = open(logfile_path, \"a\", encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            logfile = None\n",
        "\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            last_output = time.time()\n",
        "            print(line, end='')\n",
        "            if logfile:\n",
        "                try:\n",
        "                    logfile.write(line)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if not captured_url:\n",
        "                for pat in url_patterns:\n",
        "                    m = pat.search(line)\n",
        "                    if m:\n",
        "                        candidate = m.group(1).rstrip(').,\\'\"')\n",
        "                        # prefer gradio hosts\n",
        "                        if 'gradio' in candidate.lower() or ':' in candidate:\n",
        "                            captured_url = candidate\n",
        "                            break\n",
        "                        elif not captured_url:\n",
        "                            captured_url = candidate\n",
        "                if captured_url and capture_url_to:\n",
        "                    try:\n",
        "                        Path(capture_url_to).write_text(captured_url, encoding=\"utf-8\")\n",
        "                    except Exception:\n",
        "                        pass\n",
        "    except Exception as e:\n",
        "        print(f\"[stream error] {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop.set()\n",
        "        hb.join(timeout=1)\n",
        "        if logfile:\n",
        "            try:\n",
        "                logfile.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return proc.returncode, captured_url\n",
        "\n",
        "def ensure_dirs():\n",
        "    for d in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR):\n",
        "        try:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def download_repo_if_missing():\n",
        "    if WORK_DIR.exists():\n",
        "        print(f\"[info] WORK_DIR exists: {WORK_DIR}\")\n",
        "        return True\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try:\n",
        "        tmp_zip.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"[info] downloading repository...\")\n",
        "    ok = False\n",
        "    for cmd in (f\"wget -q -O {tmp_zip} {REPO_ZIP}\", f\"curl -s -L -o {tmp_zip} {REPO_ZIP}\"):\n",
        "        try:\n",
        "            sh(cmd)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            ok = True\n",
        "            break\n",
        "    if not ok:\n",
        "        print(\"[error] download failed. Check network/URL.\")\n",
        "        return False\n",
        "    print(\"[info] extracting...\")\n",
        "    try:\n",
        "        sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "        found = next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\"), None)\n",
        "        if not found:\n",
        "            print(\"[error] expected extracted folder not found\")\n",
        "            return False\n",
        "        found.rename(WORK_DIR)\n",
        "        print(\"[info] repo extracted to\", WORK_DIR)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"[error] extract failed:\", e)\n",
        "        return False\n",
        "\n",
        "def ensure_symlinks_and_files():\n",
        "    links_map = [\n",
        "        (\"models\", \"models\", False),\n",
        "        (\"loras\", \"loras\", False),\n",
        "        (\"user_data/characters\", \"characters\", False),\n",
        "        (\"user_data/presets\", \"presets\", False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\", \"chat-history\", False),\n",
        "        (\"outputs\", \"outputs\", False),\n",
        "    ]\n",
        "    for local, drive_folder, is_settings in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_folder\n",
        "        if is_settings:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                try:\n",
        "                    drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        local_path = WORK_DIR / local\n",
        "        try:\n",
        "            if local_path.exists() or local_path.is_symlink():\n",
        "                if local_path.is_symlink():\n",
        "                    local_path.unlink()\n",
        "                elif local_path.is_dir():\n",
        "                    shutil.rmtree(local_path)\n",
        "                else:\n",
        "                    local_path.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "        # attempt symlink; fallback to copy if symlink not allowed\n",
        "        try:\n",
        "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            os.symlink(str(drive_path), str(local_path), target_is_directory=drive_path.is_dir())\n",
        "        except Exception:\n",
        "            try:\n",
        "                if drive_path.is_dir():\n",
        "                    shutil.copytree(drive_path, local_path)\n",
        "                else:\n",
        "                    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    shutil.copy2(drive_path, local_path)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "def prepare_settings_file():\n",
        "    drive_settings = DRIVE_ROOT / \"settings\" / \"settings.yaml\"\n",
        "    local_settings = WORK_DIR / \"user_data\" / \"settings.yaml\"\n",
        "    local_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        if drive_settings.exists() and drive_settings.stat().st_size > 0:\n",
        "            shutil.copy2(drive_settings, local_settings)\n",
        "        else:\n",
        "            if not local_settings.exists() or local_settings.stat().st_size == 0:\n",
        "                local_settings.write_text(\"# minimal\\nlisten: true\\nshare: true\\n\", encoding=\"utf-8\")\n",
        "                drive_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "                try:\n",
        "                    shutil.copy2(local_settings, drive_settings)\n",
        "                except Exception:\n",
        "                    pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------- Main flow ----------\n",
        "ensure_dirs()\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# cleanup broken files in Drive models folder (small incomplete files)\n",
        "def cleanup_broken_files(drive_root: Path):\n",
        "    models_dir = drive_root / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        return\n",
        "    extensions = [\"*.gguf\", \"*.safetensors\", \"*.bin\", \"*.pth\", \"*.pt\"]\n",
        "    broken = []\n",
        "    for ext in extensions:\n",
        "        for f in models_dir.rglob(ext):\n",
        "            try:\n",
        "                if f.stat().st_size < (100 * 1024):\n",
        "                    broken.append(f)\n",
        "            except Exception:\n",
        "                pass\n",
        "    if broken:\n",
        "        for f in broken:\n",
        "            try:\n",
        "                f.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "cleanup_broken_files(DRIVE_ROOT)\n",
        "\n",
        "# repo\n",
        "if not download_repo_if_missing() and not WORK_DIR.exists():\n",
        "    raise SystemExit(\"Repository unavailable. Fix network/REPO_ZIP and retry.\")\n",
        "\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "ensure_symlinks_and_files()\n",
        "prepare_settings_file()\n",
        "\n",
        "# Prepare to avoid long llama.cpp compile: rename folder if present (safe)\n",
        "llama_dir = WORK_DIR / \"repositories\" / \"llama.cpp\"\n",
        "llama_disabled = None\n",
        "try:\n",
        "    if llama_dir.exists() and llama_dir.is_dir():\n",
        "        llama_disabled = llama_dir.with_name(llama_dir.name + \".disabled\")\n",
        "        if llama_disabled.exists():\n",
        "            shutil.rmtree(llama_disabled, ignore_errors=True)\n",
        "        llama_dir.rename(llama_disabled)\n",
        "        print(\"[info] renamed repositories/llama.cpp ->\", llama_disabled.name, \"(prevents auto-build)\")\n",
        "except Exception:\n",
        "    llama_disabled = None\n",
        "\n",
        "# Installer (Step 6) ‚Äî run start_linux.sh with fast flags and live streaming\n",
        "start_sh = WORK_DIR / \"start_linux.sh\"\n",
        "installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "install_env = os.environ.copy()\n",
        "install_env[\"MPLBACKEND\"] = \"Agg\"\n",
        "install_env[\"MPLCONFIGDIR\"] = str(MPL_CONFIG_DIR)\n",
        "install_env.update({\n",
        "    \"GPU_CHOICE\": \"A\",\n",
        "    \"LAUNCH_AFTER_INSTALL\": \"FALSE\",   # do not auto-launch after install\n",
        "    \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "    \"SKIP_LLAMACPP_BUILD\": \"TRUE\",     # best-effort skip of llama.cpp compile\n",
        "    \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "    \"FORCE_CUDA\": \"FALSE\",\n",
        "})\n",
        "\n",
        "print(\"\\nüì¶ Step 6/6: Installing dependencies (fast mode)...\")\n",
        "print(\"Installer log ->\", installer_log)\n",
        "sh(\"chmod +x start_linux.sh\")\n",
        "\n",
        "if env_marker.exists():\n",
        "    print(\"[info] virtualenv exists; skipping full install\")\n",
        "else:\n",
        "    if start_sh.exists():\n",
        "        code, url = stream_with_heartbeat(\"bash start_linux.sh\", cwd=str(WORK_DIR), env=install_env, logfile_path=str(installer_log), capture_url_to=str(PUBLIC_URL_FILE))\n",
        "        if code != 0:\n",
        "            print(f\"[warn] installer exited with code {code}. See {installer_log} for details.\")\n",
        "        else:\n",
        "            print(f\"[info] installer finished successfully (log: {installer_log})\")\n",
        "    else:\n",
        "        print(\"[warn] start_linux.sh not found; install skipped. You may need to run installation steps manually.\")\n",
        "\n",
        "# restore llama.cpp folder name so user can build manually later (it won't be auto-built unless script forces it)\n",
        "if llama_disabled:\n",
        "    try:\n",
        "        restored = WORK_DIR / \"repositories\" / \"llama.cpp\"\n",
        "        if not restored.exists():\n",
        "            llama_disabled.rename(restored)\n",
        "            print(\"[info] restored repositories/llama.cpp (manual build possible later)\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Ensure MPLCONFIGDIR exists and is writable, and force MPLBACKEND in launcher env\n",
        "MPL_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# LAUNCH: create small wrapper that forces matplotlib backend INSIDE server process\n",
        "wrapper = WORK_DIR / \"_run_server_with_agg.py\"\n",
        "wrapper_code = f\"\"\"# autogenerated wrapper: enforce MPL backend inside server process\n",
        "import os, runpy\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['MPLCONFIGDIR'] = r'{MPL_CONFIG_DIR}'\n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg', force=True)\n",
        "except Exception:\n",
        "    pass\n",
        "runpy.run_path('server.py', run_name='__main__')\n",
        "\"\"\"\n",
        "try:\n",
        "    wrapper.write_text(wrapper_code, encoding=\"utf-8\")\n",
        "    wrapper.chmod(0o755)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Kill stray python processes that might block ports\n",
        "try:\n",
        "    sh(\"pkill -9 -f python\")\n",
        "except Exception:\n",
        "    pass\n",
        "time.sleep(1.5)\n",
        "\n",
        "server_log = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "launch_cmd = f'{python_exe} -u \"{wrapper.name}\"'\n",
        "print(\"\\nüåê LAUNCHING UI\")\n",
        "print(\"Server log ->\", server_log)\n",
        "code, captured = stream_with_heartbeat(launch_cmd, cwd=str(WORK_DIR), env=dict(os.environ, MPLBACKEND=\"Agg\", MPLCONFIGDIR=str(MPL_CONFIG_DIR)), logfile_path=str(server_log), capture_url_to=str(PUBLIC_URL_FILE))\n",
        "\n",
        "if captured:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"[success] Public or local URL detected:\", captured)\n",
        "    try:\n",
        "        PUBLIC_URL_FILE.write_text(captured, encoding=\"utf-8\")\n",
        "        print(\"[info] URL saved to:\", PUBLIC_URL_FILE)\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    print(\"[warn] No URL captured from server output. Check server log:\", server_log)\n",
        "\n",
        "if code != 0:\n",
        "    print(f\"[warn] server exited with code {code}. See {server_log}\")\n",
        "else:\n",
        "    print(f\"[info] server terminated normally. See {server_log}\")\n",
        "\n",
        "print(\"\\n‚úì Done. Persistent data, logs and settings are in:\", DRIVE_ROOT)\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}